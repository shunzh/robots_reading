Name: Shun Zhang
EID: sz4554

== Comments and Questions ==

*. RRT

I think the setting of nonholonomic path planning only affects the
NEW_STATE function in Figure 1. The framework of algorithms is not
affected by the different settings of the environment. So I don't know
why the author classifies the planning problem into three categories
first.

The challange in this paper is that the searching space is continous.
To deal with this problem, there are at least following approaches,
- discretefy the space. This may not be a good idea, as the positions
  and actions would become less expressive.
- encourage the agent to search unexplored space - this is the
  performance of RRL. Otherwise, the agent may believe he has visited
  many states, but actually in a small subspace.
- using a RL framework. There could be a metric to describe the
  "occupation" of the explored states in the space. The reward is the
  incremental of such occupation when taking an action. Then we can
  use UCT for searching.

In RRT-GoalBias, there is a delimma of sampling a point near the goal,
or randomly. Can we use something like A* here? The performance should
be that when the states with smallest f values explored and got stuck,
the states with larger f values would autoumatically be explored.

*. D* lite

A* is okay to be applied in the environment in this paper. It's just
that it's not incremental. Its idea is also quite straight-forward -
responding to the inconsistancy. As local consistancy everywhere
implies global consistancy, it's a useful signal for where to update.

For LPA*, does it run A* once to get initial g values, and then run
LPA* forever to respond to the changes?

Is the performance of LPA* good enough to run online? If the space is
very large, ComputeShortestPath doesn't respond to new changes within
one sweep - its information is gained by UpdateVertex in the main
loop. I think there is a way to make LPA* thread and UpdateVertex
thread to operate on the priority queue simutanously.  Then they both
need to be thread-safe.

== Summary ==

*. RRT

The authors started with the setup of the environment. Apart from the
envrionment with obstacle constraint, which is holonomic path
planning, there are also nonholonomic ones.

The RRT algorithm basically makes the tree grow towards arbitrary point.

RRT-GoalBias makes the tree grow towards the goal, instead of any
direction. There is a dilemma of exploitation (making it grow towards
the goal directly) and exploration (making it grow arbitrarily,
looking for new path).

*. D* lite

Based on A*, the idea is keep track of changes edges. Theses changes
are expressed as the difference between g function and one-step-back g
function plus the one-step cost. The algorithm makes sure that the
updates go from the start state towards the goal state (or the last
states explored, if no goal defined).

The difference between D* Lite and LPA* is that D* has a goal defined
and so it can make use of that. It can also terminate if the goal is
reached. The trick is that, as the goal is static, the searching
can start from the goal and try to reach the location of the robot.

The second version of D* Lite has an improvement on the priority queue
maintaining.
