Name: Shun Zhang
EID: sz4554

== Comments and Questions ==

*. Learning and Using Models of Kicking Motions for Legged Robots 

Why don't they use a bird's-eye view camera? Then it's possible to
track the whole trajectory - including the moment when the ball is
kicked.  It would be interesting to build a belief over the position
of ball at any time, without observation, not restricted to length and
angle.

I didn't get the idea in Figure 5. What's the vertical axis?

They can use Kalman Filter to track the ball and determine the
distance and angle of the ball at t_delay more accurately. But as they
repeat the sampling many times and the noise should be un-biased, it's
okay.

*. Towards Autonomous Sensor and Actuator Model Induction on a Mobile
Robot

I think this paper describes a very interesting idea and well-deserved
for a dissertation.

"Because the world state can be estimated from independent, redundant
sources of information, the robot can use this information to learn a
model of any given individual source" (page 5, line 1)
Is "redundant" here defined more than "complete"? If the environment
has nothing more than just complete information, will ASAMI still
work?

The exploration over (state, action) space is a non-trivial task. It's
okay to unbiasedly randomly select actions. But for more efficient
exploration, it needs to get to unknown (state, action) pairs quickly. 

The authors limit the sensor model to be invertible. I think it's
possible to have a probability version of ASAMI, every function in
this paper will be a probability distribution. Then there is no such
limitation.

A general question about function approximation - can the degree of
the polynomial only be pre-selected?

Is W_a(t), the state estimation based on action model, really
incremental?  It's the sum of $A$, the action model, on every (state,
action) pair in the history. In every step $A$ is updated, should the
sum be re-calculated? Because it employs function approximation, $A$
would change globally responding to local update.

The adverse effect the authors described in page 15 is very
interesting. If I understand it correctly, it is because the update of
action model and the change of state are observed in one step. The
previous change of $A$ affects w_s, which in turn affects new $A$.

"If the learning process starts with uninitialized models, it is
likely to lead to meaningless, possibly deceptive models." (page 16,
line 1 below the caption of figure)
It's good to see ASAMI works in the experiment section. However, I
have a concern about the correctness of ASAMI. It is unlike
Q-learning, which can tolerate uninitialized models. What's the
constraints of $A_0$ to guarantee the correctness of the models?

I think this idea can be generalized to any robot with transition and
observation model, not necessarily restricted to mobile ones.

== Summary ==

*. Learning and Using Models of Kicking Motions for Legged Robots 

The authors did experiments on forward kick, left head kick and right
head kick, and recorded the statistics or angles and distances. The
Aibos can lookup the result when determine which kick to use.

*. Towards Autonomous Sensor and Actuator Model Induction on a Mobile
Robot

The authors described an algorithm that an mobile agent can learn the
transition model and observation model simultaneously. The idea is
that, they devise ways that transition or observation model can be
independently learned, assuming the other is given and perfect. Then,
they make them learn simultaneously in a bootstrapping way. 

They have many constraints in the set up of the environment. The
states must one one-dimensional, continuous. The measurement function
must be invertible. There must be an initial action model that has
rough approximation of the true one.
