Name: Shun Hanger
EID: sz4554

== Comments and Questions ==

*. Probabilistic Robotics

> Chapter 1.1 - 1.3

The example of coastal navigation shows the importance of observation,
otherwise error could accumulate. I don't see why this shows the
advantage of a probabilistic model.

Is the need to approximate shared by both model-based and
probabilistic robotics? If the world is continuous and doesn't follow
any simple distribution, approximation is the only solution?

> Chapter 4

The Kalman filter, or generally parametrized filters, can do well in
a proper subspace of the distribution space. So they are less
resource-adaptive. Nonparametric filters put their efforts on the
whole space. As a sacrifice, their approximation ability on certain
distributions is worse than parametric ones.

The discrete Bayes filter is an obvious way to apply Bayes filter to
discrete environment. The probabilities are finite, so possible to be
implemented.

For dynamic decomposition, the filter should start with a very high
resolution first? Then it's possible to combine uninteresting states.
Otherwise, information is lost in the first place..

What's the purpose of Binary Bayes filters? The author just wants to
show that it can employ log odds ratio?

I think Particle filter is just a discrete version of histogram
filter, in terms of representation of probability of states. It should
be more computationally efficient, and requires less memory.

Importance Sampling is a sampling way to attach p(z|x) to bel (with
bar). 

What are the conditions that the particle filter can fail? One
condition could be that no particles in the true state (or g(x) = 0,
g as defined in the book), and after several iterations, all particles
are weighted to 0. In the project in AI class, we just sample again
according to the prior distribution. Is there any way to use some
intermediate results?

I think density extraction is just based on reasonable guess on the
environment. No more information could be gained in this process. It's
better to divide states properly at the first place.

For the author's concern on resampling, does the variance matter?
Actually, what's the criteria for a filtering algorithm - the
difference from the result of Baysien filter (if considered as the
ideal one), or whether the most likely state is the true state?

> Chapter 7

Why the map would infer measurement directly? I think it infers
states, which then infer measurement.

Active localization involves an interesting question - what action to
take to minimize the uncertainty? This involves in-mind planning of
robot. He'll ask the question that considering all the positions I
could be in (with high probability), which action could reduce the
uncertainty (or variance on belief) the most. Also, it may take
several steps to get a good reduction on uncertainty.

So assigning this chapter is to let us get an idea of application of
Bayesian filter? Map here is like a background knowledge. The methods
in Chapter 3, 4 are still applicable.


*. Practical Vision-Based Monte Carlo Localization on a Legged Robot

The map variable is assumed to be background knowledge ant not shown
in the probability representation.

I saw there are much tuning of parameters in the model. Are they
learned by Aibo, or just intuition?

I'm not sure why the authors want to use landmark histories, or
generally observation histories, instead of using blief as a knowledge
of the history.

I think the idea in extended motion model is widely used. For PID
control, we do use a max velocity when the control exceeds a
threshold, when the distance is too far.

== Summary ==

*. Probabilistic Robotics

In chapter 1, the author gave some insights on the advantage of
employing probability in robotics. Rven if the environment is
completely predictable, the robot still needs to depend on probability
to be robust on errors - in predication or measurement.

The advantage of probabilistic robotics are that they are more robust
on predication or measurement errors, and that they require less on
accuracy of models. However, they usually require more computational
complexity. Also, as the environment is usually continuous, they need
to approximate.

In chapter 4, the author described nonparametric filters. Histogram
filters are discrete version of Bayesian filter. It's also a approach
used for continuous state spaces, by dividing them into discrete ones.
The benefit is that it becomes resource-adaptive.

Compared to Kalman filter, discrete Kalman filter and particle filter
are adaptive to any probability distribution on states. They are also
adaptive to nonlinear transitions.

Binary Baysian filter deals with the environment which contains 2
states, which should static. It employs log odds ratio, which makes
computation more elegant.

The particle filter is a popular nonparametric filter. It samples
the particles with the probability of the prior belief, then do the
following loop:
1. Transition: move each particle wrt its transition probability.
2. Observation: weight each particle wrt the observation probability.
3. Sample the particles again, but "larger" particles are drawn with
higher posterior probability.

As the author says, the particle filter is like survival of the
fittest. It is a kind of Monte-Carlo method. It simulates many agents
and only allow the ones consistent with observation survive.

It's possible to get density information by selecting a distribution,
usually Gaussian. The author also claims that higher samples result in
more accurate approximation.

In chapter 7, the author gives an example of application of Bayesian
filter - mobile localization. Its environment features are similar to
a general environment an agent could encounter (dynamic/static,
active/passive, etc.).


*. Practical Vision-Based Monte Carlo Localization on a Legged Robot

The authors give an example of application of Monte-Carlo localization
on Aibo. They show how to use vision data as measurement. Concretely,
they use similarity to expected measurement.

The contribution is that they show the improvement by recording
landmark histories, so that it works if only one landmark is observed.
For distance-based updates, they pre-process the distance data by
training a polynomial function, which maps measured value to actual
value.
